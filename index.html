<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, user-scalable=no, initial-scale=1">
  <link rel="icon" href="images/favicon.ico" type="image/x-icon" />

  <title>Stanford CS 81SI | AI Interpretability and Fairness</title>

  <!-- bootstrap -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css">

  <!-- Google fonts -->
  <link href='http://fonts.googleapis.com/css?family=Roboto:400,300' rel='stylesheet' type='text/css'>

  <!-- Google Analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-60458624-1', 'auto');
    ga('send', 'pageview');

  </script>

  <link rel="stylesheet" type="text/css" href="style.css" />

</head>

<body>
<!-- <script src="header.js"></script> -->
<!-- Navbar -->
<nav class="navbar navbar-default navbar-fixed-top">
  <div class="container">
    <div class="navbar-header">
      <a class="navbar-brand brand" href="index.html">CS81SI Home</a>
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1" aria-expanded="false">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>

    <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
      <ul class="nav navbar-nav navbar-right">
        <li><a href="index.html#schedule">Schedule</a></li>
        <li><a href="index.html#materials">Materials</a></li>
        <li><a href="office_hours.html">Office Hours</a></li>
      </ul>
    </div>
  </div>
</nav>
<!-- Header -->
<div id="header" style="text-align:center">
  <a href="http://stanford.edu/">
    <img src="images/stanfordlogo.jpg" class="logo-left">
  </a>
  <a href="http://stanford.edu/">
    <img src="images/stanfordlogo.jpg" class="logo-right">
  </a>
  <h1>CS81SI: AI Interpretability and Fairness</h1>
  <h3>Stanford, Spring 2020</h3>
  <div style="clear:both;"></div>
</div>


<!-- Logistics -->
<div class="sechighlight">
<div class="container sec" id="logistics">
  <h2>Logistics</h2>
  <ul>
    <li>Classes are at <b> Thursday 4:30-5:50pm </b>.</li>
    <li><b>Max 20 enrollment, rolling applications </b>(2 short questions, approx. 1-2 mins) <a href="https://forms.gle/Dt8h1uNtgk9RTGBPA">here</a>. Students are welcome to the first class regardless of enrollment results, but generally no auditing to keep the class Q&A and discussions small unless otherwise specified. </li>
    <li> <b> Prerequisites:</b> Students should be interested in AI research/applications, and be interested in technical and human-centric implications of AI applications. Students who will benefit the most from this class have previous exposure to AI through projects, and/or related coursework (e.g. statistics, CS221, CS229, CS 230). Undergraduate and graduate students who are pursuing subjects outside of the CS department with sufficient mathematical maturity are encouraged to apply. </li>
<!--     <li><b>Office hours</b>: Information <a href="">here</a>.</li>
 -->    <!-- <li><b>Contact</b>: Students should ask <i>all</i> course-related questions in the <a href="https://piazza.com/stanford/Spring2020/cs81SI">Piazza forum</a>, where you will also find announcements. For external enquiries, emergencies, or personal matters that you don't wish to put in a private Piazza post, you can email us at <i>cs81SI-win1920-staff@lists.stanford.edu</i>.</li> -->
<!--     <li><b>Sitting in on lectures</b>: In general we are happy for guests to sit-in on lectures if they are a member of the Stanford community (registered student, staff, and/or faculty). If the class is too full and we're running out of space, we ask that you please allow registered students to attend. Due to high enrollment, we cannot grade the work of any students who are not officially enrolled in the class.</li>
    <li><b>Academic accommodations</b>: If you need an academic accommodation based on a disability, you should initiate the request with the <a href ="https://oae.stanford.edu/accommodations/academic-accommodations">Office of Accessible Education (OAE)</a>. The OAE will evaluate the request, recommend accommodations, and prepare a letter for faculty. Students should contact the OAE as soon as possible since timely notice is needed to coordinate accommodations.</li> -->
  </ul>

  <!-- Staff Info -->
  <div class="sechighlight">
  <h2>Instructors & Organizers</h2>
    <div class="row">
        <div class="instructor">
          <a href="https://omereingold.wordpress.com/">
            <img class="headshot" src="images/omer.jpg" style="text-align:center;">
            <div style="text-align:center;">Omer Reingold</div>
          </a>
        </div>
        <div class="instructor">
          <a href="https://sites.google.com/site/jamesyzou/">
            <img class="headshot" src="images/james.jpg" style="text-align:center;">
            <div style="text-align:center;">James Zou</div>
          </a>
        </div>
        <div class="instructor">
          <a href="https://www.evazhang.com">
            <img class="headshot" src="images/eva.jpg" style="text-align:center;">
            <div style="text-align:center;">Eva Zhang</div>
          </a>
        </div>
  </div>

<!-- Content -->
<div class="sechighlight">
<div class="container sec" id="content">
  <h2>Course Description</h2>
  <h3>What is this course about?</h3>
  Questions we will explore: 
  <ol type = "1">
    <li>Why do we care about interpretable AI? Why do we care about fair AI? Real-life examples.</li>
    <li> What are model explanations? What is fair Artificial intelligence? </li>
    <li> Given a black box AI model, how do you explain it? Fundamental and State-of-the-art method taxonomies. </li>
    <li> Given an application, what type of model should you use (interpretability vs performance trade-off)? </li>
    <li> When a model is un'fair', what can you do?</li>
    <li> Regulatory trends and societal perspectives on interpretable and fair AI. </li>
  </ol>
  <h3> Course Description </h3>
  <p>
  <b>Topic: </b> As black-box AI models grow increasingly relevant in human-centric applications, explainability and fairness becomes increasingly necessary for trust in adopting AI models. This seminar class introduces students to major problems in AI explainability and fairness, and explores key state-of-the art methods to enable students to apply to future work. Key technical topics include surrogate methods, feature visualization, network dissection, adversarial debiasing, and fairness metrics. There will be a survey of recent legal and policy trends. 
  </p>
  <p> 
  From this course, students will receive an overview of some of the most important recent methods on how to explain AI models, how it's currently done, how to choose models based on level of interpretability needed, and relevant user implications of interpretability and fairness in applications such as healthcare, facial recognition, and regulatory decision-making. 
  </p> 
  <br> 
  <b> Format: </b> Each week a guest lecturer from AI research, industry, and related policy fields will present an open problem and solution, followed by a short roundtable Q&A/discussion with the class. 
  <br> 
  <br>
  <b> Participation: </b> This class is offered P/F. You should attend at least 8 of 10 sessions to receive credit. Since we keep the class size small to facilitate discussion with guest lecturers (and will generally limit auditors), participation will be taken into account. 
</div>

<!-- Coursework -->
<!-- Note the margin-top:-20px and the <br> serve to make the #coursework hyperlink display correctly (with the h2 header visible) -->
<!-- <div class="sechighlight">
<div class="container sec" id="coursework" style="margin-top:-20px">
<br>
</div> -->

<!-- Schedule -->
<!-- Note the margin-top:-20px and the <br> serve to make the #schedule hyperlink display correctly (with the h2 header visible) -->
<div class="container sec" id="schedule" style="text-align:center, margin-top:-20px">
<br>
<h2>Schedule</h2>
<p>
Speakers revealed soon! 
<table class="table">
  <colgroup>
    <col style="width:5%">
    <col style="width:25%">
    <col style="width:70%">
<!--     <col style="width:30%"> -->
  </colgroup>
  <thead>
  <tr class="active">
    <th align = "center">Week </th>
    <th align = "center"> Speaker </th>
    <th align = "center">Description</th>
<!--     <th>Course Materials</th> -->
  </tr>
  </thead>
  <tbody>
  <tr>
    <td>1</td>
    <td> </td>
    </td>
    <td>
      <b> What is AI Interpretability (& Fairness)? </b> <br>
      Why should you care? Motivation 
      Research progress, Applications, Regulatory and Ethical Considerations
      <br> 
      Definitions and desiderata of interpretability and fairness, terminology (model agnostic, whitebox vs blackbox, global vs local interpretability)
      <br> 
      Taxonomy of AI Interpretability research, major recent development summary. Tradeoff between interpretability and model accuracy. 
  </tr>
  <tr>
    <td>2</td>
    <td>  </td>
    <td>
      <b> Model-Agnostic Methods: LIME & Shapley Values </b><br>
      Overview of examples of model-agnostic methods include PDP, ICE, ALE, anchors, and Global surrogate models. 
      <br> 
    </td>
  </tr>
  <tr>
    <td>3</td>
    <td>  </td>
    <td>
      <b> Visualizing black models </b><br>
      Learned content map reconstruction, latent layer observation. 
      <br> 
      Feature Visualization and attribution methods.  
      <br> 
      Gradient-weighted class activation maps (Grad-CAMS), Network dissection. 
      <br> 
      Examples: DeConvNet, CAMS, PPGNs,and Multifaceted feature visualization. 
      <br> 
    </td>
  </tr>
  <tr>
    <td>4</td>
    <td>  </td>
    <td>
      <b> Explanations in Policy and Law </b><br>
      Recent trends and barriers to adoption in policy and law.
      <br>
      Desiderata of AI explanations, predictions. 
      <br>
      Case study on AI explanations gone wrong and right in research and industry. 
      <br>
      Interpretability application to adversarial attacks and privacy. 
      <br> 
    </td>
  </tr>
  <tr>
    <td>5</td>
    <td> </td>
    <td>
      <b> Human-Friendly Attribution </b><br>
      Concept Activation Vectors, Weakness in Salience Maps. 
      <br> 
    </td>
  </tr>
  <tr>
    <td>6</td>
    <td>  </td>
    <td>
      <b> Interpretability as a Pipeline </b><br>
      Choosing models: when to use whitebox models (decision trees, monotonic GBMs, Supersparse Linear Integer Models), performance tradeoff. 
      <br> 
      Who are explanations for? 
      <br> 
      Evaluating AI Interpretability, tradeoffs of methods mentioned in class. 
      <br> 
    </td>
  </tr>
  <tr>
    <td>7</td>
    <td>  </td>
    <td>
      <b> What is AI Fairness? Motivation, Theory, and Applications </b><br>
      Case studies and motivation (Word embedding gender biases, Amazon facial recognition, Google employment tool, COMPAS) . 
      <br> 
      Desiderata and fairness terminology. Types of bias and fairness. 
      <br> 
      Legal and Ethical Considerations in Fairness: GDPR, disparate impact and treatment examples.
      <br> 
    </td>
  </tr>
  <tr>
    <td>8</td>
    <td>  </td>
    <td>
      <b> Evaluating Fairness </b><br>
      Quantitative and qualitative fairness evaluation. 
      <br> 
      Statistical measures of discrimination. Fairness metrics example 1: Model Cards for Model Reporting. 
      <br> 
      Model calibration. 
      <br> 
    </td>
  </tr>
  <tr>
    <td>8</td>
    <td>  </td>
    <td>
      <b> Fairness Model Treatments </b><br>
      Reweighing, adversarial debiasing, and reject option-based classification.
      <br> 
      Causal fairness - Learning with imbalanced data: techniques.  
      <br> 
    </td>
  </tr>
  </tbody>
</table>



</p>
</div>

<!-- Materials -->
<!-- Note the margin-top:-20px and the <br> serve to make the #schedule hyperlink display correctly (with the h2 header visible) -->
<div class="container sec" id="materials" style="text-align:center, margin-top:-20px">
<br>
<h2>Materials</h2>
<p>
  Code examples and relevant readings coming soon! 
</p>
</div>

<!-- Website design by Chris Manning for 224N https://nlp.stanford.edu/~manning/g--> 
<hr>
<div class="container sec">
Last updated Jan 2020. Reach out to <a href="mailto:evazhang@stanford.edu">Eva</a> if there are website issues. HTML design template thanks to 224N course staff. 
</div>
</hr>

<!-- jQuery and Bootstrap -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/js/bootstrap.min.js"></script>
</body>

</html>
