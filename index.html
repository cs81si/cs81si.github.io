<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, user-scalable=no, initial-scale=1">
  <link rel="icon" href="images/favicon.ico" type="image/x-icon" />

  <title>Stanford CS 81SI | AI Interpretability and Fairness</title>

  <!-- bootstrap -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css">

  <!-- Google fonts -->
  <link href='http://fonts.googleapis.com/css?family=Roboto:400,300' rel='stylesheet' type='text/css'>

  <!-- Google Analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-60458624-1', 'auto');
    ga('send', 'pageview');

  </script>

  <link rel="stylesheet" type="text/css" href="style.css" />

</head>

<body>
<!-- <script src="header.js"></script> -->
<!-- Navbar -->
<nav class="navbar navbar-default navbar-fixed-top">
  <div class="container">
    <div class="navbar-header">
      <a class="navbar-brand brand" href="index.html">CS81SI Home</a>
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1" aria-expanded="false">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>

    <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
      <ul class="nav navbar-nav navbar-right">
        <li><a href="index.html#coursework">Coursework</a></li>
        <li><a href="index.html#schedule">Schedule</a></li>
        <li><a href="office_hours.html">Office Hours</a></li>
      </ul>
    </div>
  </div>
</nav>

<!-- Header -->
<div id="header" style="text-align:center">
  <a href="http://stanford.edu/">
    <img src="images/stanfordlogo.jpg" class="logo-left">
  </a>
  <a href="http://stanford.edu/">
    <img src="images/stanfordlogo.jpg" class="logo-right">
  </a>
  <h1>CS81SI: AI Interpretability and Fairness</h1>
  <h3>Stanford / Spring 2020</h3>
  <div style="clear:both;"></div>
</div>


<!-- Logistics -->
<div class="sechighlight">
<div class="container sec" id="logistics">
  <h2>Logistics</h2>
  <ul>
    <li><b>Seminars</b> are on Thursday 4:30-5:50pm Pacific Time in <a href="https://goo.gl/maps/hRjQYd6MqxB2">TBD.
    <li><b>Office hours</b>: Information <a href="office_hours.html">here</a>.</li>
    <li><b>Contact</b>: Students should ask <i>all</i> course-related questions in the <a href="https://piazza.com/stanford/Spring2020/cs81SI">Piazza forum</a>, where you will also find announcements. For external enquiries, emergencies, or personal matters that you don't wish to put in a private Piazza post, you can email us at <i>cs81SI-win1920-staff@lists.stanford.edu</i>.</li>
    <li><b>Sitting in on lectures</b>: In general we are happy for guests to sit-in on lectures if they are a member of the Stanford community (registered student, staff, and/or faculty). If the class is too full and we're running out of space, we ask that you please allow registered students to attend. Due to high enrollment, we cannot grade the work of any students who are not officially enrolled in the class.</li>
    <li><b>Academic accommodations</b>: If you need an academic accommodation based on a disability, you should initiate the request with the <a href ="https://oae.stanford.edu/accommodations/academic-accommodations">Office of Accessible Education (OAE)</a>. The OAE will evaluate the request, recommend accommodations, and prepare a letter for faculty. Students should contact the OAE as soon as possible since timely notice is needed to coordinate accommodations.</li>
  </ul>

  <!-- Staff Info -->
  <div class="row">
      <h3>Instructors</h3>
      <div class="instructor">
        <a href="https://omereingold.wordpress.com/">
        <div class="instructorphoto"><img src="images/omer.jpg"></div>
        <div>Omer Reingold</div>
        </a>
        <a href="https://www.james-zou.com/">
        <div class="instructorphoto"><img src="images/james.jpg"></div>
        <div>James Zou</div>
        </a>
      <h3>Student Instructor</h3>
      <div class="instructor">
        <a href="https://www.evazhang.com/">
        <div class="instructorphoto"><img src="images/eva.jpg"></div>
        <div>Eva Zhang</div>
        </a>
  </div>

<!-- Content -->
<div class="container sec" id="content">
  <h2>Content</h2>
  <h3>What is this course about?</h3>
  <p>
<!--     Natural language processing (NLP) is one of the most important technologies of the information age, and a crucial part of artificial intelligence.
    Applications of NLP are everywhere because people communicate almost everything in language: web search, advertising, emails, customer service, language translation, virtual agents, medical reports, etc.
    In recent years, Deep Learning approaches have obtained very high performance across many different NLP tasks, using single end-to-end neural models that do not require traditional, task-specific feature engineering.
    In this course, students will gain a thorough introduction to cutting-edge research in Deep Learning for NLP.
    Through lectures, assignments and a final project, students will learn the necessary skills to design, implement, and understand their own neural network models.
    As piloted last year, CS81SI will be taught using <a href="https://pytorch.org"><b>PyTorch</b></a> this year. -->
  </p>

  <h3>Prerequisites</h3>
  <ul>
      <li><b>Proficiency in Python</b>
          <p>All class assignments will be in Python (using NumPy and PyTorch). If you need to remind yourself of Python, or you're not very familiar with NumPy, you can come to the Python review session in week 1 (listed in the <a href="#schedule">schedule</a>). If you have a lot of programming experience but in a different language (e.g. C/C++/Matlab/Java/Javascript), you will probably be fine.</p>
      </li>
      <li><b>College Calculus, Linear Algebra</b> (e.g. MATH 51, CME 100)
          <p>You should be comfortable taking (multivariable) derivatives and understanding matrix/vector notation and operations.</p>
      </li>
      <li><b>Basic Probability and Statistics</b> (e.g. CS 109 or equivalent)
          <p>You should know basics of probabilities, gaussian distributions, mean, standard deviation, etc.</p>
      </li>
      <li><b>Foundations of Machine Learning</b> (e.g. CS 221 or CS 229)
          <p>We will be formulating cost functions, taking derivatives and performing optimization with gradient descent.
            If you already have basic machine learning and/or deep learning knowledge, the course will be easier; however it is possible to take CS81SI without it. There are many introductions to ML, in webpage, book, and video form. One approachable introduction is Hal Daum&eacute;'s in-progress <a href="http://ciml.info"><i>A Course in Machine Learning</i></a>. Reading the first 5 chapters of that book would be good background. Knowing the first 7 chapters would be even better!</p>
      </li>
  </ul>
  <h3>Reference Texts</h3>
  <p>
    The following texts are useful, but not required. All of them can be read free online.
  </p>
  <ul>
      <li>
          Dan Jurafsky and James H. Martin. <a href="https://web.stanford.edu/~jurafsky/slp3/">Speech and Language Processing (3rd ed. draft)</a>
    </li>
      <li>
          Jacob Eisenstein. <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Natural Language Processing</a>
      </li>
      <li>
          Yoav Goldberg. <a href="http://u.cs.biu.ac.il/~yogo/nnlp.pdf">A Primer on Neural Network Models for Natural Language Processing</a>
      </li>
      <li>
          Ian Goodfellow, Yoshua Bengio, and Aaron Courville. <a href="http://www.deeplearningbook.org/">Deep Learning</a>
      </li>
      <li>
          Delip Rao and Brian McMahan. <a href="http://library.stanford.edu/sfx?genre=book&atitle=&title=Natural%20language%20processing%20with%20PyTorch%20:%20build%20intelligent%20language%20applications%20using%20deep%20learning%20/&isbn=9781491978207&volume=&issue=&date=20190101&aulast=Rao,%20Delip,,%20author.&spage=&pages=&sid=EBSCO:VLeBooks:edsvle.AH35866319">Natural Language Processing with PyTorch</a>. (requires Stanford login)
      </li>
    </ul>
    <p>
    If you have no background in neural networks but would like to take the course anyway, you might well find one of these books helpful to give you more background:
    </p>
    <ul>
      <li>
         Michael A. Nielsen. <a href="http://neuralnetworksanddeeplearning.com">Neural Networks and Deep Learning</a>
      </li>
      <li>
      Eugene Charniak. <a href="https://mitpress.mit.edu/books/introduction-deep-learning">Introduction to Deep Learning</a>
      </li>
  </ul>
</div>

<!-- Coursework -->
<!-- Note the margin-top:-20px and the <br> serve to make the #coursework hyperlink display correctly (with the h2 header visible) -->
<div class="sechighlight">
<div class="container sec" id="coursework" style="margin-top:-20px">
<br>
<h2>Coursework</h2>
  <h3>Assignments (54%)</h3>
  <p>
    There are five weekly assignments, which will improve both your theoretical understanding and your practical skills. All assignments contain both written questions and programming parts.
  </p>
  <ul>
    <li><b>Credit</b>:
      <ul>
        <li>Assignment 1 (6%): Introduction to word vectors</li>
        <li>Assignment 2 (12%): Derivatives and implementation of word2vec algorithm</li>
        <li>Assignment 3 (12%): Dependency parsing and neural network foundations</li>
        <li>Assignment 4 (12%): Neural Machine Translation with sequence-to-sequence and attention</li>
        <li>Assignment 5 (12%): Neural Machine Translation with ConvNets and subword modeling</li>
      </ul>
    <li><b>Deadlines</b>: All assignments are due on either a Tuesday or a Thursday <i>before class</i> (i.e. before 4:30pm). All deadlines are listed in the <a href="#schedule">schedule</a>.</li>
    <li><b>Submission</b>: Assignments are submitted via <a href="https://www.gradescope.com/courses/77592">Gradescope</a>. If you need to sign up for a Gradescope account, please use your <u>@stanford.edu</u> email address. Further instructions are given in each assignment handout.
    <i>Do not email us your assignments</i>.</li>
    <li><b>Collaboration</b>:
    Study groups are allowed, but students must understand and complete their own assignments, and hand in one assignment per student.
    If you worked in a group, please put the names of the members of your study group at the top of your assignment.
    Please ask if you have any questions about the collaboration policy.
    </li>
    <li><b>Honor Code</b>:
    We expect students to not look at solutions or implementations online. Like all other classes at Stanford, we take the student <a href="https://ed.stanford.edu/academics/masters-handbook/honor-code">Honor Code</a> seriously.
    </li>
  </ul>

  <h3>Final Project (43%)</h3>
    <p>
      The Final Project offers you the chance to apply your newly acquired skills towards an in-depth application.
      Students have two options: the <b>Default Final Project</b> (in which students tackle a predefined task, namely textual Question Answering) or a <b>Custom Final Project</b> (in which students choose their own project). Examples of both can be seen on <a href="https://web.stanford.edu/class/archive/cs/cs81SI/cs81SI.1194/project.html">last year's website</a>.
    </p>
    <h4>Important information</h4>
    <ul>
      <li><b>Credit</b>: For both default and custom projects, credit for the final project is broken down as follows:
        <ul>
          <li>
            Project proposal (5%) [<a href="project/project-proposal-instructions.pdf">instructions</a>]
          </li>
          <li>
            Project milestone (5%) [<a href="project/project-milestone-instructions.pdf">instructions</a>]
          </li>
          <li>
            Project poster/video (3%) [<a href="project/project-postervideo-instructions.pdf">instructions</a>]
          </li>
          <li>
            Project report (30%) [<a href="project/project-report-instructions.pdf">instructions</a>]
          </li>
        </ul>
      </li>
      <li><b>Deadlines</b>: The project proposal, milestone and report are all due at 4:30pm. All deadlines are listed in the <a href="#schedule">schedule</a>.</li>
      <li><b>Default Final Project</b> [<a href="project/default-final-project-handout.pdf">handout</a>] [<a href="slides/cs81SI-2019-lecture10-QA.pdf">lecture slides</a>]: In this project, students explore deep learning solutions to the <a href="https://rajpurkar.github.io/SQuAD-explorer/">SQuAD (Stanford Question Asking Dataset) challenge</a>.
      This year's project is similar to last year's, on SQuAD 2.0 with baseline code in PyTorch.
      </li>
      <li><b>Project advice</b> [<a href="slides/cs81SI-2019-lecture09-final-projects.pdf">lecture slides</a>] [<a href="readings/final-project-practical-tips.pdf">lecture notes</a>]: The <i>Practical Tips for Final Projects</i> lecture provides guidance for choosing and planning your project.
      To get project advice from staff members, first look at each staff member's areas of expertise on the <a href="office_hours.html#staff">office hours page</a>. This should help you find a staff member who is knowledgable about your project area.
      </li>
      <!--<li><b>Project ideas from Stanford researchers</b>: We have collected a list of <a href="https://docs.google.com/document/d/1Ytncuq6tpiSGHsJBkdzskMf0nw4_x2AJ1rZ7RvpOv5E/edit?usp=sharing">project ideas</a> from members of the Stanford AI Lab &mdash; these are a great opportunity to work on an interesting research problem with an external mentor. If you want to do these, get started early!</li>-->
    </ul>

  <h3>Participation (3%)</h3>
  <p>
    We appreciate everyone being actively involved in the class! There are several ways of earning participation credit, which is capped at 3%:
  </p>
  <ul>
    <li><b>Attending guest speakers' lectures</b>:</li>
      <ul>
        <li>In the second half of the class, we have three invited speakers. Our guest speakers make a significant effort to come lecture for us, so (both to show our appreciation and to continue attracting interesting speakers) we do not want them lecturing to a largely empty room.</li>
        <li>For on-campus students, your attendance at lectures with guest speakers is expected! You will get 0.5% per speaker (1.5% total) for attending.</li>
        <li>Since SCPD students can’t (easily) attend classes, they can instead get 0.83% per speaker (2.5% total) by writing a ‘reaction paragraph’ based on listening to the talk; details will be provided. Non-SCPD students with an unavoidable absence <i>who ask in advance via a private Piazza post</i> can also do this option. Non-SCPD students with a clashing course may only submit this request if the other course is a lab, or is not recorded.</li>
      </ul>
    <li><b>Attending two random lectures</b>: At two randomly-selected (non-guest) lectures in the quarter, we will take attendance. Each is worth 0.5% (total 1%).</li>
    <li><b>Completing feedback surveys</b>: We will send out two feedback surveys (mid-quarter and end-of-quarter) to help us understand how the course is going, and how we can improve. Each of the two surveys are worth 0.5%.</li>
    <li><b>Piazza participation</b>: The top ~20 contributors to Piazza will get 3%; others will get credit in proportion to the participation of the ~20th person.</li>
    <li><b>Karma point</b>: Any other act that improves the class, which a CS81SI TA or instructor notices and deems worthy: 1%</li>
  </ul>
</div>
</div>

<!-- Schedule -->
<!-- Note the margin-top:-20px and the <br> serve to make the #schedule hyperlink display correctly (with the h2 header visible) -->
<div class="container sec" id="schedule" style="margin-top:-20px">
<br>
<h2>Schedule</h2>
<p>
  Updated lecture <b>slides</b> will be posted here shortly before each lecture. The current links contain last year's slides, which are mostly similar.
</p>
<p>
  The lecture <b>notes</b> are updated versions of the CS81SI 2017 lecture notes (viewable <a href="https://web.stanford.edu/class/archive/cs/cs81SI/cs81SI.1174/syllabus.html">here</a>) and will be uploaded a few days after each lecture. The notes (which cover approximately the first half of the course content) give supplementary detail beyond the lectures.
</p>
<table class="table">
  <colgroup>
    <col style="width:10%">
    <col style="width:20%">
    <col style="width:40%">
    <col style="width:10%">
    <col style="width:10%">
  </colgroup>
  <thead>
  <tr class="active">
    <th>Date</th>
    <th>Description</th>
    <th>Course Materials</th>
    <th>Events</th>
    <th>Deadlines</th>
  </tr>
  </thead>
  <tbody>
  <tr>
    <td>Tue Jan 7</td>
    <td>Introduction and Word Vectors
      <br>
      [<a href="slides/cs81SI-2020-lecture01-wordvecs1.pdf">slides</a>]
      [<a href="https://stanford-pilot.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=b2acdfb7-8038-49fb-941e-ab25012bd9ec">video</a>]
      [<a href="readings/cs81SI-2019-notes01-wordvecs1.pdf">notes</a>]
      <br><br>
      Gensim word vectors example:
      <br>
      [<a href="materials/Gensim.zip">code</a>]
      [<a href="materials/Gensim%20word%20vector%20visualization.html">preview</a>]
    </td>
    <td>
      Suggested Readings:
      <ol>
        <li><a href=http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/>Word2Vec Tutorial - The Skip-Gram Model</a></li>
        <li><a href="http://arxiv.org/pdf/1301.3781.pdf">Efficient Estimation of Word Representations in Vector Space</a> (original word2vec paper)</li>
        <li><a href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">Distributed Representations of Words and Phrases and their Compositionality</a> (negative sampling paper)</li>
      </ol>
    </td>
    <td>
      Assignment 1 <b><font color="green">out</font></b>
      <br>
      [<a href="assignments/a1.zip">code</a>]
      [<a href="assignments/a1_preview/exploring_word_vectors.html">preview</a>]
    </td>
    <td></td>
  </tr>

  <tr>
    <td>Thu Jan 9</td>
    <td>Word Vectors 2 and Word Senses
      <br>
      [<a href="slides/cs81SI-2020-lecture02-wordvecs2.pdf">slides</a>]
      [<a href="https://stanford-pilot.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=68213ef7-af13-4e96-b154-ab25012bda54">video</a>]
      [<a href="readings/cs81SI-2019-notes02-wordvecs2.pdf">notes</a>]
    </td>
    <td>
      Suggested Readings:
      <ol>
        <li><a href="http://nlp.stanford.edu/pubs/glove.pdf">GloVe: Global Vectors for Word Representation</a> (original GloVe paper)</li>
        <li><a href="http://www.aclweb.org/anthology/Q15-1016">Improving Distributional Similarity with Lessons Learned from Word Embeddings</a></li>
        <li><a href="http://www.aclweb.org/anthology/D15-1036">Evaluation methods for unsupervised word embeddings</a></li>
      </ol>
      Additional Readings:
      <ol>
        <li><a href="http://aclweb.org/anthology/Q16-1028">A Latent Variable Model Approach to PMI-based Word Embeddings</a></li>
        <li><a href="https://transacl.org/ojs/index.php/tacl/article/viewFile/1346/320">Linear Algebraic Structure of Word Senses, with Applications to Polysemy</a></li>
        <li><a href="https://papers.nips.cc/paper/7368-on-the-dimensionality-of-word-embedding.pdf">On the Dimensionality of Word Embedding.</a></li>
      </ol>
    </td>
    <td></td>
    <td></td>
  </tr>

  <tr class="warning">
    <td>Fri Jan 10</td>
    <td>Python review session
      <br>
      [<a href="readings/cs81SI-python-review-20.pdf">slides</a>]
      [<a href="https://stanford.box.com/s/50i2a4w29adk0vfc075cuguv3tlu4t1x">video</a>]
      [<a href="readings/python-review-demo.py">code</a>]
    </td>
    <td>
      2:30 - 4:20pm<br>160-124 [<a href="https://campus-map.stanford.edu/">map</a>]
    </td>
    <td></td>
    <td></td>
  </tr>

  <tr>
    <td>Tue Jan 14</td>
    <td>Word Window Classification, Neural Networks, and PyTorch
      <br>
      [<a href="slides/cs81SI-2020-lecture03-neuralnets.pdf">slides</a>]
      [<a href="https://stanford-pilot.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=bd799d65-6da2-4bef-b075-ab25012bda8c">video</a>]
      [<a href="materials/ww_classifier.ipynb">code (notebook)</a>]
      [<a href="materials/ww_classifier.slides.html">code (html)</a>]
      <br>
      [<a href="readings/gradient-notes.pdf">matrix calculus notes</a>]
      <br>
      [<a href="readings/cs81SI-2019-notes03-neuralnets.pdf">notes (lectures 3 and 4)</a>]
    </td>
    <td>
      Suggested Readings:
      <ol>
        <li><a href="readings/review-differential-calculus.pdf">Review of differential calculus</a></li>
      </ol>
      Additional Readings:
      <ol>
        <li><a href="http://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf">Natural Language Processing (Almost) from Scratch</a></li>
      </ol>
    </td>
    <td>
      Assignment 2 <b><font color="green">out</font></b>
      <br>
      [<a href="assignments/a2.zip">code</a>]
      [<a href="assignments/a2.pdf">handout</a>]
    </td>
    <td>Assignment 1 <b><font color="red">due</font></b></td>
  </tr>

  <tr>
    <td>Thu Jan 16</td>
    <td>Matrix Calculus and Backpropagation
      <br>
      [<a href="slides/cs81SI-2020-lecture04-neuralnets.pdf">slides</a>]
      [<a href="https://stanford-pilot.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=0661dbaa-3726-40f7-89d2-ab25012bdab6">video</a>]
      <br>
      [<a href="readings/cs81SI-2019-notes03-neuralnets.pdf">notes (lectures 3 and 4)</a>]
    </td>
    <td>
      Suggested Readings:
      <ol>
        <li><a href="http://cs231n.github.io/neural-networks-1/">CS231n notes on network architectures</a></li>
        <li><a href="http://cs231n.github.io/optimization-2/">CS231n notes on backprop</a></li>
        <li><a href="http://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf">Learning Representations by Backpropagating Errors</a></li>
        <li><a href="http://cs231n.stanford.edu/handouts/derivatives.pdf">Derivatives, Backpropagation, and Vectorization</a></li>
        <li><a href="https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b">Yes you should understand backprop</a></li>
      </ol>
      </td>
      <td></td>
      <td></td>
  </tr>

  <tr>
    <td>Tue Jan 21</td>
    <td>Linguistic Structure: Dependency Parsing
      <br>
      [<a href="slides/cs81SI-2020-lecture05-dep-parsing.pdf">slides</a>]
      <br>
      [<a href="https://stanford-pilot.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=0607b355-903d-4b4b-ad42-ab25012bdadc">video</a>]
      [<a href="readings/cs81SI-2019-notes04-dependencyparsing.pdf">notes</a>]
     </td>
    <td>
      Suggested Readings:
      <ol>
        <li><a href="https://www.aclweb.org/anthology/W/W04/W04-0308.pdf">Incrementality in Deterministic Dependency Parsing</a></li>
        <li><a href="https://www.emnlp2014.org/papers/pdf/EMNLP2014082.pdf">A Fast and Accurate Dependency Parser using Neural Networks</a></li>
        <li><a href="http://www.morganclaypool.com/doi/abs/10.2200/S00169ED1V01Y200901HLT002">Dependency Parsing</a></li>
        <li><a href="https://arxiv.org/pdf/1603.06042.pdf">Globally Normalized Transition-Based Neural Networks</a></li>
        <li><a href="http://nlp.stanford.edu/~manning/papers/USD_LREC14_UD_revision.pdf">Universal Stanford Dependencies: A cross-linguistic typology</li>
        <li><a href="http://universaldependencies.org/">Universal Dependencies website</a></li>
      </ol>
    </td>
    <td>Assignment 3 <b><font color="green">out</font></b>
        <br>
        [<a href="assignments/a3.zip">code</a>]
        [<a href="assignments/a3.pdf">handout</a>]
    </td>
    <td>Assignment 2 <b><font color="red">due</font></b></td>
  </tr>

  <tr>
    <td>Thu Jan 23</td>
    <td>The probability of a sentence? Recurrent Neural Networks and Language Models
      <br>
      [<a href="slides/cs81SI-2020-lecture06-rnnlm.pdf">slides</a>]
      [<a href="https://youtu.be/iWea12EAu6U">video</a>]
      <br>
      [<a href="readings/cs81SI-2019-notes05-LM_RNN.pdf">notes (lectures 6 and 7)</a>]
    </td>

    <td>
      Suggested Readings:
      <ol>
        <li><a href="https://web.stanford.edu/~jurafsky/slp3/3.pdf">N-gram Language Models</a> (textbook chapter)</li>
        <li><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a> (blog post overview)</li>
        <!-- <li><a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/">Recurrent Neural Networks Tutorial</a> (practical guide)</li> -->
        <li><a href="http://www.deeplearningbook.org/contents/rnn.html">Sequence Modeling: Recurrent and Recursive Neural Nets</a> (Sections 10.1 and 10.2)</li>
  <li><a href="http://norvig.com/chomsky.html">On Chomsky and the Two Cultures of Statistical Learning</a>
      </ol>
    </td>
    <td></td>
    <td></td>
  </tr>

  <tr>
    <td>Tue Jan 28</td>
    <td>Vanishing Gradients and Fancy RNNs
      <br>
      [<a href="slides/cs81SI-2019-lecture07-fancy-rnn.pdf">slides</a>]
      [<a href="https://youtu.be/QEw0qEa0E50">video</a>]
      <br>
      [<a href="readings/cs81SI-2019-notes05-LM_RNN.pdf">notes (lectures 6 and 7)</a>]
    </td>
    <td>
      Suggested Readings:
      <ol>
        <li><a href="http://www.deeplearningbook.org/contents/rnn.html">Sequence Modeling: Recurrent and Recursive Neural Nets</a> (Sections 10.3, 10.5, 10.7-10.12)</li>
        <li><a href="http://ai.dinfo.unifi.it/paolo//ps/tnn-94-gradient.pdf">Learning long-term dependencies with gradient descent is difficult</a> (one of the original vanishing gradient papers)</li>
        <li><a href="https://arxiv.org/pdf/1211.5063.pdf">On the difficulty of training Recurrent Neural Networks</a> (proof of vanishing gradient problem)</li>
        <li><a href="https://web.stanford.edu/class/archive/cs/cs81SI/cs81SI.1174/lectures/vanishing_grad_example.html">Vanishing Gradients Jupyter Notebook</a> (demo for feedforward networks)</li>
        <li><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTM Networks</a> (blog post overview)</li>
        <!-- <li><a href="https://arxiv.org/pdf/1504.00941.pdf">A simple way to initialize recurrent networks of rectified linear units</a></li> -->
      </ol>
    </td>
    <td>Assignment 4 <b><font color="green">out</font></b>
        <!--
        <br>
        [<a href="assignments/a4.zip">code</a>]
        [<a href="assignments/a4.pdf">handout</a>]
        [<a href="https://docs.google.com/document/d/1MHaQvbtPkfEGc93hxZpVhkKum1j_F1qsyJ4X0vktUDI/edit">Azure Guide</a>]
        [<a href="https://docs.google.com/document/d/1z9ST0IvxHQ3HXSAOmpcVbFU5zesMeTtAc9km6LAPJxk/edit">Practical Guide to VMs</a>]
        -->
    </td>
    <td>Assignment 3 <b><font color="red">due</font></b></td>
  </tr>

  <tr>
    <td>Thu Jan 30</td>
    <td>Machine Translation, Seq2Seq and Attention
      <br>
      [<a href="slides/cs81SI-2019-lecture08-nmt.pdf">slides</a>]
      [<a href="https://youtu.be/XXtpJxZBa2c">video</a>]
      [<a href="readings/cs81SI-2019-notes06-NMT_seq2seq_attention.pdf">notes</a>]
    </td>
    <td>
      Suggested Readings:
      <ol>
        <li><a href="https://web.stanford.edu/class/archive/cs/cs81SI/cs81SI.1162/syllabus.shtml">Statistical Machine Translation slides, CS81SI 2015</a> (lectures 2/3/4)</li>
        <li><a href="https://www.cambridge.org/core/books/statistical-machine-translation/94EADF9F680558E13BE759997553CDE5">Statistical Machine Translation</a> (book by Philipp Koehn)</li>
        <li><a href="https://www.aclweb.org/anthology/P02-1040.pdf">BLEU</a> (original paper)</li>
        <li><a href="https://arxiv.org/pdf/1409.3215.pdf">Sequence to Sequence Learning with Neural Networks</a> (original seq2seq NMT paper)</a></li>
        <li><a href="https://arxiv.org/pdf/1211.3711.pdf">Sequence Transduction with Recurrent Neural Networks</a> (early seq2seq speech recognition paper)</li>
        <li><a href="https://arxiv.org/pdf/1409.0473.pdf">Neural Machine Translation by Jointly Learning to Align and Translate</a> (original seq2seq+attention paper)</li>
        <li><a href="https://distill.pub/2016/augmented-rnns/">Attention and Augmented Recurrent Neural Networks</a> (blog post overview)</li>
        <li><a href="https://arxiv.org/pdf/1703.03906.pdf">Massive Exploration of Neural Machine Translation Architectures</a> (practical advice for hyperparameter choices)</li>
      </ol>
    </td>
    <td></td>
    <td></td>
  </tr>

  <tr>
    <td>Tue Feb 4</td>
    <td>
      Practical Tips for Final Projects
      <br>
      [<a href="slides/cs81SI-2019-lecture09-final-projects.pdf">slides</a>]
      [<a href="https://youtu.be/fyqm8fRDgl0">video</a>]
      [<a href="readings/final-project-practical-tips.pdf">notes</a>]
    </td>
    <td>
      Suggested Readings:
      <ol>
        <li><a href="https://www.deeplearningbook.org/contents/guidelines.html">Practical Methodology</a> (<i>Deep Learning</i> book chapter)</li>
      </ol>
    </td>
    <td>Project Proposal <b><font color="green">out</font></b>
        <br>
        [<a href="project/project-proposal-instructions.pdf">instructions</a>]
        <br><br>
        Default Final Project <b><font color="green">out</font></b>
        [<a href="project/default-final-project-handout.pdf">handout</a>]
        <!-- [<a href="https://github.com/chrischute/squad">code</a>] -->
    </td>
    <td></td>
  </tr>

  <tr>
    <td>Thu Feb 6</td>
    <td>Question Answering and the Default Final Project<br>
    [<a href="slides/cs81SI-2019-lecture10-QA.pdf">slides</a>]
    [<a href="https://youtu.be/yIdF-17HwSk">video</a>]
    [<a href="readings/cs81SI-2019-notes07-QA.pdf">notes</a>]
  </td>
    <td>
      Suggested Readings:
      <ol>
        <li><a href="http://web.stanford.edu/class/cs81SI/project/default-final-project-handout.pdf">Project Handout</a>
        </li>
      </ol>
    </td>
    <td></td>
    <td>Assignment 4 <b><font color="red">due</font></b></td>
  </tr>

  <tr>
    <td>Tue Feb 11</td>
    <td>ConvNets for NLP <br>
    [<a href="slides/cs81SI-2019-lecture11-convnets.pdf">slides</a>]
    [<a href="https://youtu.be/EAJoRA0KX7I">video</a>]
    [<a href="readings/cs81SI-2019-notes08-CNN.pdf">notes</a>]
    </td>
    <td>
      Suggested Readings:
      <ol>
        <!-- <li><a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a></li>
        <li><a href="https://arxiv.org/pdf/1607.06450.pdf">Layer Normalization</a></li> -->
        <li><a href="https://arxiv.org/abs/1408.5882">Convolutional Neural Networks for Sentence Classification</a></li>
        <!-- <li><a href="https://arxiv.org/abs/1207.0580">Improving neural networks by preventing co-adaptation of feature detectors</a></li> -->
        <li><a href="https://arxiv.org/pdf/1404.2188.pdf">A Convolutional Neural Network for Modelling Sentences</a></li>
      </ol>
    </td>
    <td></td>
    <td>Project Proposal <b><font color="red">due</font></b></td>
  </tr>

  <tr>
    <td>Thu Feb 13</td>
    <td>Information from parts of words (Subword Models) and Transformer architectures
    <br>
    [<a href="slides/cs81SI-2019-lecture12-subwords.pdf">slides</a>]
    [<a href="https://youtu.be/9oTHFx0Gg3Q">video</a>]
    </td>
    <td>Suggested readings:
      <ol>
      <li>
           Minh-Thang Luong and Christopher Manning. <a href="https://arxiv.org/abs/1604.00788">Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models</a>
      </li>
        <li>
          <a href="https://arxiv.org/pdf/1808.09943.pdf">Revisiting Character-Based Neural Machine Translation with Capacity and Compression</a>
        </li>
      </ol>
    </td>
    <td>
      Assignment 5 <b><font color="green">out</font></b>
      <!--
      <br>
      [<a href="https://stanford.box.com/s/t4nlmcc08t9k6mflz6sthjlmjs7lip6p">original code (requires Stanford login)</a> / <a href="assignments/a5_public.zip">public version</a>]
      [<a href="assignments/a5.pdf">handout</a>]
      -->
    </td>
    <td></td>
  </tr>

  <tr>
    <td>Tue Feb 18</td>
    <td>Contextual Word Representations: BERT <i>(guest lecture by <a href="https://research.google/people/106320/">Jacob Devlin</a>)</i></td>
    <td>Suggested readings:
      <ol>
        <li>
          <a href="https://arxiv.org/pdf/1810.04805.pdf">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a>
        </li>
      </ol>
    </td>
    <td></td>
    <td></td>
  </tr>

  <tr>
    <td>Thu Feb 20</td>
    <td>Modeling contexts of use: Contextual Representations and Pretraining
      <br>
      [<a href="slides/cs81SI-2019-lecture13-contextual-representations.pdf">slides</a>]
      [<a href="https://youtu.be/S-CspeZ8FHc">video</a>]
    </td>
    <td>Suggested readings:
      <ol>
         <li>
           <a href="https://arxiv.org/abs/1902.06006">Contextual Word Representations: A Contextual Introduction</a>.
         </li>
   <li><a href="http://jalammar.github.io/illustrated-bert/">The
   Illustrated BERT, ELMo, and co.</a>
   </li>
      </ol>
    </td>
    <td>
      Project Milestone <b><font color="green">out</font></b>
    </td>
    <td>Assignment 5 <b><font color="red">due</font></b></td>
  </tr>

  <!--
  <tr>
    <td>Thu Feb 20</td>
    <td>
      Transformers and Self-Attention For Generative Models
      <br>
      <i>(guest lecture by <a href="https://ai.google/research/people/AshishVaswani">Ashish Vaswani</a> and <a href="https://ai.google/research/people/105787">Anna Huang</a>)</i>
      <br>
      [<a href="slides/cs81SI-2019-lecture14-transformers.pdf">slides</a>]
      [<a href="https://youtu.be/5vcj8kSwBCY">video</a>]
    </td>
    <td>Suggested readings:
      <ol>
         <li><a href="https://arxiv.org/pdf/1706.03762.pdf">Attention is all you need</a></li>
         <li><a href="https://arxiv.org/pdf/1802.05751.pdf">Image Transformer</a></li>
         <li><a href="https://arxiv.org/pdf/1809.04281.pdf">Music Transformer: Generating music with long-term structure</a></li>
      </ol>
    </td>
    <td></td>
    <td></td>
  </tr>
  

  <tr class="warning">
    <td>Fri Feb 21</td>
    <td></td>
    <td></td>
    <td>
      Project Milestone <b><font color="green">out</font></b>
    </td>
    <td>Assignment 5 <b><font color="red">due</font></b></td>
  </tr>
  -->
  <tr>
    <td>Tue Feb 25</td>
    <td>
      Natural Language Generation
      <br>
      [<a href="slides/cs81SI-2019-lecture15-nlg.pdf">slides</a>]
      [<a href="https://youtu.be/4uG1NMKNWCU">video</a>]
    </td>
    <td></td>
    <td></td>
    <td></td>
  </tr>

  <tr>
    <td>Thu Feb 27</td>
    <td>Reference in Language and Coreference Resolution
      <br>
      [<a href="slides/cs81SI-2019-lecture16-coref.pdf">slides</a>]
      [<a href="https://youtu.be/i19m4GzBhfc">video</a>]
    </td>
    <td></td>
    <td></td>
    <td></td>
  </tr>

  <tr>
    <td>Tue Mar 3</td>
    <td>Fairness and Inclusion in AI <i>(guest lecture by <a href="https://www.cs.stanford.edu/~vinod/">Vinodkumar Prabhakaran</a>)</i>
    </td>
    <td></td>
    <td></td>
    <td>Project Milestone <b><font color="red">due</font></b></td>
  </tr>

  <tr>
    <td>Thu Mar 5</td>
    <td>
      Constituency Parsing and Tree Recursive Neural Networks
      <br>
      [<a href="slides/cs81SI-2019-lecture18-TreeRNNs.pdf">slides</a>]
      [<a href="https://youtu.be/6Z4A3RSf-HY">video</a>]
      [<a href="readings/cs81SI-2019-notes09-RecursiveNN_constituencyparsing.pdf">notes</a>]
    </td>
    <td>
      Suggested Readings:
      <ol>
        <li><a href="http://www.aclweb.org/anthology/P13-1045">Parsing with Compositional Vector Grammars.</a></li>
        <li><a href="https://arxiv.org/pdf/1805.01052.pdf">Constituency Parsing with a Self-Attentive Encoder</a></li>
      </ol>
    </td>
    <td></td>
    <td></td>
  </tr>

  <tr>
    <td>Tue Mar 10</td>
    <td>Recent Advances in Low Resource Machine Translation <i>(guest lecture by <a href="https://ranzato.github.io">Marc'Aurelio Ranzato</a>)</i>
    </td>
    <td></td>
    <td></td>
    <td></td>
  </tr>

  <tr>
    <td>Thu Mar 12</td>
    <td>
      Future of NLP + Deep Learning
      <br>
      [<a href="slides/cs81SI-2019-lecture20-future.pdf">slides</a>]
      [<a href="https://youtu.be/3wWZBGN-iX8">video</a>]
    </td>
    <td></td>
    <td></td>
    <td></td>
  </tr>

  <tr class="warning">
    <td>Sun Mar 15</td>
    <td></td>
    <td></td>
    <td></td>
    <td>
      <b>Final Project Report <font color="red">due</font></b>
      <!--
      [<a href="project/project-report-instructions.pdf">instructions</a>]
      -->
    </td>
  </tr>

  <tr class="warning">
    <td>Mon Mar 16</td>
    <td><b>Final project poster session</b>
    <br>
    <!--
    [<a href="https://www.facebook.com/events/1218481914969541">details</a>]
    -->
    </td>
    <td>5:30 - 10pm <br>McCaw Hall at the Alumni Center [<a href="https://alumni.stanford.edu/get/page/resources/alumnicenter/directions">map</a>]
    </td>
    <td></td>
    <td>
      <b>Project Poster/Video <font color="red">due</font></b>
      <!--
      [<a href="project/project-postervideo-instructions.pdf">instructions</a>]
      -->
    </td>
  </tr>
  </tbody>
</table>
</div>

<!-- jQuery and Bootstrap -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/js/bootstrap.min.js"></script>
</body>

</html>
